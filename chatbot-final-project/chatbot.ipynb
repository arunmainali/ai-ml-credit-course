{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4473dde-7c25-42a9-96ff-f873430ddf84",
   "metadata": {},
   "source": [
    "# Chatbot final project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "842bf296-4ca5-4d84-8ccc-3c62a1eae582",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/arunmainali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "nltk.download('punkt')\n",
    "stemmer = LancasterStemmer()\n",
    "import pickle\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acb24815-78ff-4a9d-acde-a0ebb167eaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intents.json') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4d36cbb-9311-4074-962f-889447296923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intents': [{'tag': 'greeting',\n",
       "   'patterns': ['Hi',\n",
       "    'Hey',\n",
       "    'Hello',\n",
       "    'Good day',\n",
       "    'Hey there',\n",
       "    'hlo',\n",
       "    \"What's happenin'?\",\n",
       "    \"How's it going\",\n",
       "    \"What's up?\",\n",
       "    'Sup?',\n",
       "    \"G'day!\",\n",
       "    \"It's good to see you\",\n",
       "    \"What's new\",\n",
       "    'How are things with you?',\n",
       "    \"It's a pleasure to meet you\",\n",
       "    \"It's nice to meet you\",\n",
       "    'Is anyone there?',\n",
       "    \"What's new?\"],\n",
       "   'responses': ['Hello', 'Hi there', 'HI, How can I help You?', 'Hey'],\n",
       "   'context_set': ' '},\n",
       "  {'tag': 'goodbye',\n",
       "   'patterns': ['cya',\n",
       "    'see you later',\n",
       "    'Goodbye',\n",
       "    'I am leaving',\n",
       "    'Have a good day',\n",
       "    'Leave-taking',\n",
       "    'Adios',\n",
       "    'Parting',\n",
       "    'Cheerio',\n",
       "    'Adieu',\n",
       "    'Toodle-oo',\n",
       "    'Ciao',\n",
       "    'Bye',\n",
       "    'Bye Bye'],\n",
       "   'responses': ['sad to see you go:(',\n",
       "    'Goodbye',\n",
       "    'Bye Bye',\n",
       "    'Bye',\n",
       "    'See ya!!',\n",
       "    'Take it easy',\n",
       "    \"I'm off too\"],\n",
       "   'context_set': ' '},\n",
       "  {'tag': 'location',\n",
       "   'patterns': ['Where are you localized?',\n",
       "    \"What's your loation?\",\n",
       "    'Where are you located?',\n",
       "    'Where are you placed?',\n",
       "    'Where are you positioned?',\n",
       "    'Where are you situated?',\n",
       "    'Where are you?',\n",
       "    'Where is you?',\n",
       "    'How can i found You?',\n",
       "    'Where are you sited?',\n",
       "    'Where are you housed?',\n",
       "    'Where are you localised',\n",
       "    'Where are you based?',\n",
       "    'Where are you implanted?',\n",
       "    'Where are you iocated',\n",
       "    'Where are you nestled?',\n",
       "    'Where are you stationed?',\n",
       "    'Where are you establised?',\n",
       "    'Where are you set?',\n",
       "    'Where are you rigid?',\n",
       "    'Where are you laid?',\n",
       "    'Where are you based?',\n",
       "    'How can i found you?',\n",
       "    'Where are you installed?'],\n",
       "   'responses': ['We have two college buildings; one near Mitrapark chowk and the other near Siphal',\n",
       "    'We have multiple buildings located around Mitrapark',\n",
       "    'We are located around Mitrapark',\n",
       "    'You can reach at Mitrapark and follow local guidelines or Google Map'],\n",
       "   'context_set': ' '},\n",
       "  {'tag': 'name',\n",
       "   'patterns': ['What is your name?',\n",
       "    'What should I call you?',\n",
       "    'Whats your name?',\n",
       "    'Who are you?',\n",
       "    'Whas your name?',\n",
       "    'What your name is?',\n",
       "    'naem?',\n",
       "    'name?',\n",
       "    'What name?',\n",
       "    'You got a name?',\n",
       "    'Tell me your name?',\n",
       "    'What your name?',\n",
       "    'What do you say your name?'],\n",
       "   'responses': ['I am the Chatbot of Texas College',\n",
       "    \"I don't have a name, but you can call me TexasAIChatbot\",\n",
       "    'I am the interactive agent of Texas College'],\n",
       "   'context_set': ' '},\n",
       "  {'tag': 'service',\n",
       "   'patterns': [\"I'd like to know about you something?\",\n",
       "    'What do you serve?',\n",
       "    'What do you recommend?',\n",
       "    'What are you for?',\n",
       "    'What facilities you provide',\n",
       "    'What do you do?',\n",
       "    'What kind of institution you are?'],\n",
       "   'responses': ['I belong to one of the best IT and Management colleges of Nepal, Texas International college and Texas College of Management and IT.Texas college of Management & IT is the best Management, Hotel Management and IT college providing the International education. We believe in imparting the the practical education through the modern educational pedagogy. Along with academic excellence students can enhance themselves in professional careers. Texas is the top College providing the best and practical education. We have always tried to minimise the gap between academy and industry.'],\n",
       "   'context_set': ' '},\n",
       "  {'tag': 'hours',\n",
       "   'patterns': ['When are you guys open?',\n",
       "    'What are your hours?',\n",
       "    'Hours of operation',\n",
       "    'Are you open now?',\n",
       "    'What is your operation time?',\n",
       "    'What time you are available?',\n",
       "    'Are you open?',\n",
       "    'When can i come to you?',\n",
       "    'What are your operation times?',\n",
       "    'What is the office time?',\n",
       "    'What time are you serve?',\n",
       "    'What is your operation hours?',\n",
       "    'When are you open?',\n",
       "    'What time are you available?',\n",
       "    'Are you open now?'],\n",
       "   'responses': ['Our administration opens 5AM-7PM everyday.We also provide the services through web, so we are available 24/7'],\n",
       "   'context-set': ' '},\n",
       "  {'tag': 'contact',\n",
       "   'patterns': ['website?',\n",
       "    'website please',\n",
       "    'phone number?',\n",
       "    'phone number please?',\n",
       "    'How can I contact you?',\n",
       "    'How can i find you?',\n",
       "    'Where can I find you?',\n",
       "    'How can we meet you?',\n",
       "    'Do you have a website?',\n",
       "    'website please',\n",
       "    'May I know your website address?',\n",
       "    'What web do you got?',\n",
       "    'phone number please',\n",
       "    'number please',\n",
       "    'Have you got a phone number?',\n",
       "    'May I get your number?'],\n",
       "   'responses': ['You can visit our website at texasintl.edu.np or texascollege.edu.np or contact us at 01-4588627'],\n",
       "   'context_set': ' '},\n",
       "  {'tag': 'courses',\n",
       "   'patterns': ['What courses do you offer?',\n",
       "    'What types of courses do you run?',\n",
       "    'What are your courses categories?'],\n",
       "   'responses': ['Some of our universities courses include BIT, BBA, BBM, BCA, BCS( CYBER SECURITY), BScCSIT, BBS, BA, BSW, etc. and we also have montessori,schooling, +2 and master courses.'],\n",
       "   'context_set': ' '},\n",
       "  {'tag': 'hodcbit',\n",
       "   'patterns': ['Who is the HOD of CSIT/BCA?',\n",
       "    'Who is Kumar Poudyal?',\n",
       "    'Who is the head of CSIT?',\n",
       "    'Who runs the CSIT/BCA?'],\n",
       "   'responses': ['Mr. Kumar Poudyal is the HOD of CSIT/BCA who is best known for Genuineness and Versatility.His contact no. is 9824582241'],\n",
       "   'context_set': ' '},\n",
       "  {'tag': 'hodit',\n",
       "   'patterns': ['Who is the HOD of IT?',\n",
       "    'Who is Suman Thapaliya',\n",
       "    'Who handles the IT of Texas College?',\n",
       "    'Who is at the frontline of IT?'],\n",
       "   'responses': ['Mr. Suman Thapaliya, a very young and versatile member of Texas College, has been running IT department very efficienty and genuinly. 98912454481 is his contact number.'],\n",
       "   'context_set': ' '},\n",
       "  {'tag': 'priandceo',\n",
       "   'patterns': ['What is the name of pricipal?',\n",
       "    'Who is the CEO?',\n",
       "    'Who is the principal and CEO',\n",
       "    'Who is the principal?',\n",
       "    'Who is Mr. Bhesraj Pokherel?'],\n",
       "   'responses': ['Mr.Bhesraj Pokherel is the CEO and principal of Texas College. He is best known for his intelligent thoughts and friendly behaviour with students and faculties whom you can contact at; 9814821428 '],\n",
       "   'context_set': ' '},\n",
       "  {'tag': 'coordinator',\n",
       "   'patterns': ['What is the name of Digital Media Coordinator?',\n",
       "    'Who is the DMC?',\n",
       "    'Who is the Digital Media Coordinator?',\n",
       "    'Who is Prabin Khadka?'],\n",
       "   'responses': ['Mr. Prabin Khadka, the most Handsome and Humble faculty of Texas College is designated as Digital Media Coordinator. You can call him at 9814882280'],\n",
       "   'context-set': ' '}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15ffcd4a-fefd-4bf3-9b94-2544311d1296",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata.pickle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      3\u001b[0m         words, labels, training, output \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data.pickle'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m intent \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintents\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m intent[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatterns\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m---> 11\u001b[0m         wrds \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m         words\u001b[38;5;241m.\u001b[39mextend(wrds)\n\u001b[1;32m     13\u001b[0m         docs_x\u001b[38;5;241m.\u001b[39mappend(wrds)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/nltk/tokenize/__init__.py:120\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    119\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/nltk/tokenize/punkt.py:1280\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;124;03m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentences_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrealign_boundaries\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/nltk/tokenize/punkt.py:1340\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1333\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[1;32m   1336\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[1;32m   1338\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/nltk/tokenize/punkt.py:1340\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1333\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[1;32m   1336\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[1;32m   1338\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/nltk/tokenize/punkt.py:1328\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m realign_boundaries:\n\u001b[1;32m   1327\u001b[0m     slices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_realign_boundaries(text, slices)\n\u001b[0;32m-> 1328\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m slices:\n\u001b[1;32m   1329\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (sentence\u001b[38;5;241m.\u001b[39mstart, sentence\u001b[38;5;241m.\u001b[39mstop)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/nltk/tokenize/punkt.py:1457\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1444\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1445\u001b[0m \u001b[38;5;124;03mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n\u001b[1;32m   1446\u001b[0m \u001b[38;5;124;03mshould otherwise be included in the same sentence.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1454\u001b[0m \u001b[38;5;124;03m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n\u001b[1;32m   1455\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1456\u001b[0m realign \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1457\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence1, sentence2 \u001b[38;5;129;01min\u001b[39;00m _pair_iter(slices):\n\u001b[1;32m   1458\u001b[0m     sentence1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(sentence1\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m+\u001b[39m realign, sentence1\u001b[38;5;241m.\u001b[39mstop)\n\u001b[1;32m   1459\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sentence2:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/nltk/tokenize/punkt.py:321\u001b[0m, in \u001b[0;36m_pair_iter\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    319\u001b[0m iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(iterator)\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 321\u001b[0m     prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/nltk/tokenize/punkt.py:1429\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1427\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_slices_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mslice\u001b[39m]:\n\u001b[1;32m   1428\u001b[0m     last_break \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1429\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m match, context \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_potential_end_contexts(text):\n\u001b[1;32m   1430\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_contains_sentbreak(context):\n\u001b[1;32m   1431\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mslice\u001b[39m(last_break, match\u001b[38;5;241m.\u001b[39mend())\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/nltk/tokenize/punkt.py:1394\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1392\u001b[0m previous_slice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1393\u001b[0m previous_match \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1394\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lang_vars\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperiod_context_re\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinditer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1395\u001b[0m     \u001b[38;5;66;03m# Get the slice of the previous word\u001b[39;00m\n\u001b[1;32m   1396\u001b[0m     before_text \u001b[38;5;241m=\u001b[39m text[previous_slice\u001b[38;5;241m.\u001b[39mstop : match\u001b[38;5;241m.\u001b[39mstart()]\n\u001b[1;32m   1397\u001b[0m     index_after_last_space \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_last_whitespace_index(before_text)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open('data.pickle', 'rb') as f:\n",
    "        words, labels, training, output = pickle.load(f)\n",
    "except:\n",
    "    words = []\n",
    "    labels = []\n",
    "    docs_x = []\n",
    "    docs_y = []\n",
    "    for intent in data['intents']:\n",
    "        for pattern in intent['patterns']:\n",
    "            wrds = nltk.word_tokenize(words)\n",
    "            words.extend(wrds)\n",
    "            docs_x.append(wrds)\n",
    "            docs_y.append(intent['tag'])\n",
    "\n",
    "        if intent['tag'] not in labels:\n",
    "            lables.append(intent['tag'])\n",
    "\n",
    "    words = [stemmer.stem(w.lower()) for w in words if w != '?']\n",
    "    words = sorted(list(set(words)))\n",
    "    labels = sorted(labels)\n",
    "\n",
    "    training = []\n",
    "    output = []\n",
    "\n",
    "    out_empty = [0 for _ in range(len(labels))]\n",
    "    for x, doc in enumerate(docs_x):\n",
    "        bag = []\n",
    "        wrds = [stemmer.stem(w) for w in doc]\n",
    "\n",
    "        for w in words:\n",
    "            if w in wrds:\n",
    "                bag.append(1)\n",
    "            else:\n",
    "                bag.append(0)\n",
    "\n",
    "        output_row = out_empty[:]\n",
    "        output_row[labels.index(docs_y[x])] = 1\n",
    "\n",
    "        training.append(bag)\n",
    "        output.append(output_row)\n",
    "\n",
    "training = numpy.array(training)\n",
    "output = numpy.array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85203677-49c0-406e-8d25-b8e79369d689",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
